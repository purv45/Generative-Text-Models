{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, set_seed\n",
        "\n",
        "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "set_seed(42)\n",
        "\n",
        "# Example prompt\n",
        "prompt = \"The impact of artificial intelligence on healthcare\"\n",
        "output = generator(prompt, max_length=150, num_return_sequences=1)\n",
        "\n",
        "print(\"‚úÖ GPT-2 Generated Paragraph:\\n\")\n",
        "print(output[0]['generated_text'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M53h_HJ0Zg8q",
        "outputId": "3ee18d3d-cedb-4f5b-8d1c-5d56a74b652a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ GPT-2 Generated Paragraph:\n",
            "\n",
            "The impact of artificial intelligence on healthcare is certainly massive. But that will only ever change if we believe in something called AI ‚Äì that is, the ability to control individuals who are smarter than us?\n",
            "\n",
            "In the US, this argument involves an unmitigated moral failure to address the growing potential that AI will replace humans. Despite our best efforts, such a philosophy exists only in theory: this is largely a fantasy, and it exists to serve its theoretical purposes, and to serve the interests of the corporate capitalist class. We are doing our best to avoid the problem, but this is the case only by making an artificial society with many different types ‚Äì with many different demographics, to varying degrees ‚Äì that can be used to solve complex problems. As such\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "# üìò Longer sample paragraph for better learning\n",
        "text = \"\"\"\n",
        "Artificial intelligence is transforming industries by automating tasks, improving efficiency, and enabling data-driven decisions.\n",
        "Its applications in healthcare, education, and transportation are reshaping the way we live and work.\n",
        "AI-powered tools are becoming increasingly integrated into daily life, driving innovation and new business models.\n",
        "\"\"\"\n",
        "\n",
        "# Character mapping\n",
        "chars = sorted(set(text))\n",
        "char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
        "ix_to_char = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = len(chars)\n",
        "hidden_size = 128\n",
        "seq_length = 40\n",
        "lr = 0.01\n",
        "\n",
        "# Data preparation\n",
        "def create_sequences(text, seq_length):\n",
        "    X, y = [], []\n",
        "    for i in range(len(text) - seq_length):\n",
        "        seq = text[i:i+seq_length]\n",
        "        target = text[i+1:i+seq_length+1]\n",
        "        X.append([char_to_ix[ch] for ch in seq])\n",
        "        y.append([char_to_ix[ch] for ch in target])\n",
        "    return X, y\n",
        "\n",
        "X, y = create_sequences(text, seq_length)\n",
        "X = torch.tensor(X)\n",
        "y = torch.tensor(y)\n",
        "\n",
        "# Define the LSTM model\n",
        "class LSTMTextGen(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(LSTMTextGen, self).__init__()\n",
        "        self.embed = nn.Embedding(input_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size)\n",
        "        self.fc = nn.Linear(hidden_size, input_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        x = self.embed(x)\n",
        "        out, hidden = self.lstm(x, hidden)\n",
        "        out = self.fc(out)\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return (torch.zeros(1, 1, hidden_size), torch.zeros(1, 1, hidden_size))\n",
        "\n",
        "model = LSTMTextGen(input_size, hidden_size)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# Training the model\n",
        "print(\"\\n‚è≥ Training LSTM...\")\n",
        "for epoch in range(100):\n",
        "    for i in range(len(X)):\n",
        "        input_seq = X[i].unsqueeze(1)\n",
        "        target_seq = y[i].unsqueeze(1)\n",
        "\n",
        "        hidden = model.init_hidden()\n",
        "        optimizer.zero_grad()\n",
        "        output, _ = model(input_seq, hidden)\n",
        "        loss = loss_fn(output.view(-1, input_size), target_seq.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Text generation function\n",
        "def generate_text(model, start_str, length=500, temperature=1.0):\n",
        "    model.eval()\n",
        "    hidden = model.init_hidden()\n",
        "    input_seq = torch.tensor([[char_to_ix[ch] for ch in start_str]]).T\n",
        "    result = start_str\n",
        "\n",
        "    for _ in range(length):\n",
        "        output, hidden = model(input_seq, hidden)\n",
        "        last_char_logits = output[-1]\n",
        "        probs = F.softmax(last_char_logits, dim=1).data.squeeze().pow(1 / temperature)\n",
        "        probs = probs / probs.sum()\n",
        "        char_ind = torch.multinomial(probs, 1)[0]\n",
        "        char = ix_to_char[char_ind.item()]\n",
        "        result += char\n",
        "        input_seq = torch.tensor([[char_ind]])\n",
        "\n",
        "    return result\n",
        "\n",
        "# Generate paragraph using trained LSTM\n",
        "print(\"\\n‚úÖ LSTM Generated Paragraph:\\n\")\n",
        "print(generate_text(model, \"Artificial\", length=1000, temperature=1.0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOrPUquRY72K",
        "outputId": "2790207e-4005-4423-89bb-c39f77a101da"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚è≥ Training LSTM...\n",
            "Epoch 0, Loss: 0.4457\n",
            "Epoch 10, Loss: 0.0648\n",
            "Epoch 20, Loss: 0.0287\n",
            "Epoch 30, Loss: 0.0302\n",
            "Epoch 40, Loss: 0.0169\n",
            "Epoch 50, Loss: 0.0367\n",
            "Epoch 60, Loss: 0.0329\n",
            "Epoch 70, Loss: 0.0495\n",
            "Epoch 80, Loss: 0.0092\n",
            "Epoch 90, Loss: 0.0546\n",
            "\n",
            "‚úÖ LSTM Generated Paragraph:\n",
            "\n",
            "Artificial intellligence is transportation and new business modells are becoming increasinablintransportation and new business models.\n",
            "AI-powered tools are becomincreasin healthcare becoming increasinablthcare becoming increasinablinttrated into daily life, driving innovation and new business models.\n",
            "AI-powered toools are becomincy, and work.\n",
            "AI-powered tools are becoming increasinabling data-driven decision,shapingly integly integrated into daily life, driving innovation and new business models.\n",
            "AI-powered toools areforming increasinabling data-driven decisions.\n",
            "Its is is is integrated into daily life, driving innovation and new business modells.\n",
            "AI-powered toools abusfoming increasin healthcare becoming increasinablinttrated into daily life, driving innovation and new business modells.\n",
            "AI-powered tools are becoming increasin healthcare becoming increasinabling data-driven decisions.\n",
            "Its is is integrated into daily life, driving innovation and new business models.\n",
            "AI-powered tools are becoming \n"
          ]
        }
      ]
    }
  ]
}